{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1e56af",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357bbf1-d472-4f66-bbb1-519c4ad41e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config & helpers ---\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO, SeqRecord, Seq\n",
    "import pandas as pd, numpy as np\n",
    "import os, re, shutil, subprocess\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Paths\n",
    "BASE        = Path(\"/~/HMM-PROFILES-FOR-CA-ENZYMES\")\n",
    "OG          = BASE / \"OG_Labels\"                 # original per-class FASTAs (*.txt)\n",
    "NEW_LABELS  = BASE / \"new_labels\"                # <class>_<n>_seqdump.txt\n",
    "REBASE      = BASE / \"OUTPUT\"                   # workspace\n",
    "RE_OG_UPD   = REBASE / \"OG_Labels_UPDATED\"       # OUTPUT: merged + dedup per-class FASTAs\n",
    "SPLIT_SRC   = REBASE / \"per_class_split_sources\" # recursively scanned\n",
    "\n",
    "# Outputs for analysis\n",
    "OUT_DIR   = REBASE / \"length_consensus_outputs\"\n",
    "ALIGN_DIR = OUT_DIR / \"aligned\"\n",
    "CONS_DIR  = OUT_DIR / \"consensus\"\n",
    "HM_DIR    = OUT_DIR / \"heatmaps\"\n",
    "for d in [REBASE, RE_OG_UPD, OUT_DIR, ALIGN_DIR, CONS_DIR, HM_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def first_token(s: str) -> str:\n",
    "    return s.split()[0] if s else s\n",
    "\n",
    "def load_fasta_records(path: Path):\n",
    "    recs = []\n",
    "    for r in SeqIO.parse(str(path), \"fasta\"):\n",
    "        sid = first_token(r.id or r.description)\n",
    "        r.id = sid; r.description = \"\"\n",
    "        recs.append(r)\n",
    "    return recs\n",
    "\n",
    "def write_fasta(records, path: Path):\n",
    "    if records:\n",
    "        SeqIO.write(records, str(path), \"fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc012653-a38a-4ca1-86e2-0e79e5f21f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed UPDATED from OG (ensures every class file exists)\n",
    "for f in sorted(OG.glob(\"*.txt\")):\n",
    "    dst = RE_OG_UPD / f.name\n",
    "    if not dst.exists():\n",
    "        shutil.copy2(f, dst)\n",
    "\n",
    "# Index current IDs\n",
    "existing_ids = {}\n",
    "for f in sorted(RE_OG_UPD.glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    ids = { first_token(r.id or r.description) for r in SeqIO.parse(str(f), \"fasta\") }\n",
    "    existing_ids[cls] = set(ids)\n",
    "\n",
    "# Infer class from split filenames or parent dir\n",
    "class_pat_list = [\n",
    "    re.compile(r'^(?P<cls>[A-Za-z]+)[._-]'),        # alpha.something.fa\n",
    "    re.compile(r'8ca[._-](?P<cls>[A-Za-z]+)[._-]'), # 8ca.zeta.45.*\n",
    "    re.compile(r'^(?P<cls>[A-Za-z]+)$'),            # plain stem\n",
    "]\n",
    "def infer_class_from_path(p: Path):\n",
    "    stem = p.stem\n",
    "    for pat in class_pat_list:\n",
    "        m = pat.search(stem)\n",
    "        if m: return m.group(\"cls\").lower()\n",
    "    m2 = re.match(r'^(?P<cls>[A-Za-z]+)$', p.parent.name)\n",
    "    return m2.group(\"cls\").lower() if m2 else None\n",
    "\n",
    "# Gather split files\n",
    "split_files = []\n",
    "if SPLIT_SRC.exists():\n",
    "    for ext in (\"*.fa\",\"*.fasta\",\"*.faa\",\"*.fas\",\"*.txt\"):\n",
    "        split_files += list(SPLIT_SRC.rglob(ext))\n",
    "\n",
    "# Merge SPLIT sources\n",
    "for f in sorted(split_files):\n",
    "    cls_simple = infer_class_from_path(f)\n",
    "    if not cls_simple: \n",
    "        continue\n",
    "    targets = [p for p in RE_OG_UPD.glob(\"*.txt\")\n",
    "               if re.search(rf'\\b{re.escape(cls_simple)}\\b', p.stem, flags=re.IGNORECASE)]\n",
    "    if not targets:\n",
    "        targets = [RE_OG_UPD / f\"8ca.{cls_simple}.extra.txt\"]\n",
    "        targets[0].touch(exist_ok=True)\n",
    "\n",
    "    new_recs = load_fasta_records(f)\n",
    "    for out_path in targets:\n",
    "        key = out_path.stem\n",
    "        prev = existing_ids.setdefault(key, set())\n",
    "        current = list(SeqIO.parse(str(out_path), \"fasta\"))\n",
    "        add = 0\n",
    "        seen_local = set()\n",
    "        for r in new_recs:\n",
    "            if r.id in seen_local: \n",
    "                continue\n",
    "            seen_local.add(r.id)\n",
    "            if r.id not in prev:\n",
    "                current.append(r); prev.add(r.id); add += 1\n",
    "        write_fasta(current, out_path)\n",
    "\n",
    "# Merge NEW_LABELS dumps\n",
    "dump_re = re.compile(r'^(?P<cls>[A-Za-z]+)_(?P<num>\\d+)_seqdump\\.txt$')\n",
    "if NEW_LABELS.exists():\n",
    "    for f in sorted(NEW_LABELS.glob(\"*.txt\")):\n",
    "        m = dump_re.match(f.name)\n",
    "        if not m: \n",
    "            continue\n",
    "        cls_simple = m.group(\"cls\").lower()\n",
    "        targets = [p for p in RE_OG_UPD.glob(\"*.txt\")\n",
    "                   if re.search(rf'\\b{re.escape(cls_simple)}\\b', p.stem, flags=re.IGNORECASE)]\n",
    "        if not targets:\n",
    "            targets = [RE_OG_UPD / f\"8ca.{cls_simple}.extra.txt\"]\n",
    "            targets[0].touch(exist_ok=True)\n",
    "\n",
    "        new_recs = load_fasta_records(f)\n",
    "        for out_path in targets:\n",
    "            key = out_path.stem\n",
    "            prev = existing_ids.setdefault(key, set())\n",
    "            current = list(SeqIO.parse(str(out_path), \"fasta\"))\n",
    "            add = 0\n",
    "            seen_local = set()\n",
    "            for r in new_recs:\n",
    "                if r.id in seen_local: \n",
    "                    continue\n",
    "                seen_local.add(r.id)\n",
    "                if r.id not in prev:\n",
    "                    current.append(r); prev.add(r.id); add += 1\n",
    "            write_fasta(current, out_path)\n",
    "\n",
    "# Quick summary\n",
    "summary = []\n",
    "for f in sorted(RE_OG_UPD.glob(\"*.txt\")):\n",
    "    n = sum(1 for _ in SeqIO.parse(str(f), \"fasta\"))\n",
    "    summary.append((f.name, n))\n",
    "pd.DataFrame(summary, columns=[\"class_file\",\"n_sequences\"]).sort_values(\"class_file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d6a52",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d24bdc-7956-491e-8d43-1a3563930197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_len(rec: SeqRecord) -> int:\n",
    "    return sum(1 for ch in str(rec.seq) if ch not in \"-.\")\n",
    "\n",
    "def trimmed_mean(arr, lo=0.25, hi=0.75):\n",
    "    if not arr: return float(\"nan\")\n",
    "    a = np.array(sorted(arr))\n",
    "    i0, i1 = int(np.floor(len(a)*lo)), int(np.ceil(len(a)*hi))\n",
    "    mid = a[i0:i1]\n",
    "    return float(np.mean(mid)) if len(mid) else float(\"nan\")\n",
    "\n",
    "def trimmed_median(arr, lo=0.25, hi=0.75):\n",
    "    if not arr: return float(\"nan\")\n",
    "    a = np.array(sorted(arr))\n",
    "    i0, i1 = int(np.floor(len(a)*lo)), int(np.ceil(len(a)*hi))\n",
    "    mid = a[i0:i1]\n",
    "    return float(np.median(mid)) if len(mid) else float(\"nan\")\n",
    "\n",
    "rows = []\n",
    "all_lengths = []\n",
    "for f in sorted(RE_OG_UPD.glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    lens = [seq_len(r) for r in SeqIO.parse(str(f), \"fasta\")]\n",
    "    all_lengths.extend(lens)\n",
    "    rows.append({\n",
    "        \"class\": cls,\n",
    "        \"n_seqs\": len(lens),\n",
    "        \"mean_len\": np.mean(lens) if lens else float(\"nan\"),\n",
    "        \"median_len\": np.median(lens) if lens else float(\"nan\"),\n",
    "        \"std_len\": np.std(lens, ddof=1) if len(lens) > 1 else float(\"nan\"),\n",
    "        \"trimmed_mean_len_25_75\": trimmed_mean(lens),\n",
    "        \"trimmed_median_len_25_75\": trimmed_median(lens),\n",
    "        \"min_len\": int(np.min(lens)) if lens else None,\n",
    "        \"q25_len\": int(np.quantile(lens, 0.25)) if lens else None,\n",
    "        \"q75_len\": int(np.quantile(lens, 0.75)) if lens else None,\n",
    "        \"max_len\": int(np.max(lens)) if lens else None,\n",
    "    })\n",
    "\n",
    "rows.append({\n",
    "    \"class\": \"__ALL__\",\n",
    "    \"n_seqs\": sum(r[\"n_seqs\"] for r in rows),\n",
    "    \"mean_len\": np.mean(all_lengths) if all_lengths else float(\"nan\"),\n",
    "    \"median_len\": np.median(all_lengths) if all_lengths else float(\"nan\"),\n",
    "    \"std_len\": np.std(all_lengths, ddof=1) if len(all_lengths) > 1 else float(\"nan\"),\n",
    "    \"trimmed_mean_len_25_75\": trimmed_mean(all_lengths),\n",
    "    \"trimmed_median_len_25_75\": trimmed_median(all_lengths),\n",
    "    \"min_len\": int(np.min(all_lengths)) if all_lengths else None,\n",
    "    \"q25_len\": int(np.quantile(all_lengths, 0.25)) if all_lengths else None,\n",
    "    \"q75_len\": int(np.quantile(all_lengths, 0.75)) if all_lengths else None,\n",
    "    \"max_len\": int(np.max(all_lengths)) if all_lengths else None,\n",
    "})\n",
    "\n",
    "df_stats = pd.DataFrame(rows).sort_values(\"class\")\n",
    "df_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb147aa-8d3b-41c6-8d7f-7f2815fc6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECDF per class + explicit overall, with mean markers\n",
    "\n",
    "IN_DIR = RE_OG_UPD\n",
    "SAVE_PNG = OUT_DIR / \"length_ecdf_per_class.png\"\n",
    "INCLUDE_OVERALL = True\n",
    "MAX_CLASSES_PLOTTED = None\n",
    "USE_LOG_X = False\n",
    "SHOW_MEANS = True  # add vertical lines at class means\n",
    "SHOW_OVERALL_MEAN = True\n",
    "\n",
    "def seq_len_fast(r): return sum(1 for ch in str(r.seq) if ch not in \"-.\")\n",
    "def ecdf(vals):\n",
    "    if len(vals)==0: return np.array([]), np.array([])\n",
    "    x = np.sort(np.asarray(vals))\n",
    "    y = (np.arange(1, len(x)+1) / len(x)) * 100.0\n",
    "    return x, y\n",
    "\n",
    "# gather\n",
    "lengths_by_class = {}\n",
    "all_lengths = []\n",
    "for f in sorted(Path(IN_DIR).glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    lens = [seq_len_fast(r) for r in SeqIO.parse(str(f), \"fasta\")]\n",
    "    if lens:\n",
    "        lengths_by_class[cls] = np.array(lens, dtype=int)\n",
    "        all_lengths.extend(lens)\n",
    "\n",
    "# optional limit\n",
    "if MAX_CLASSES_PLOTTED is not None and len(lengths_by_class) > MAX_CLASSES_PLOTTED:\n",
    "    keep = dict(sorted(lengths_by_class.items(), key=lambda kv: len(kv[1]), reverse=True)[:MAX_CLASSES_PLOTTED])\n",
    "    lengths_by_class = keep\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# plot classes\n",
    "handles = []\n",
    "labels  = []\n",
    "for cls, lens in sorted(lengths_by_class.items()):\n",
    "    x, p = ecdf(lens)\n",
    "    if len(x)==0: continue\n",
    "    h = ax.step(x, p, where=\"post\", linewidth=1.3, label=f\"{cls} (n={len(lens)})\")[0]\n",
    "    handles.append(h); labels.append(h.get_label())\n",
    "    if SHOW_MEANS:\n",
    "        ax.axvline(np.mean(lens), color=h.get_color(), linestyle=\":\", linewidth=1.0, alpha=0.8)\n",
    "\n",
    "# plot overall explicitly with fixed styling & label\n",
    "if INCLUDE_OVERALL and len(all_lengths)>0:\n",
    "    x_all, p_all = ecdf(all_lengths)\n",
    "    hov = ax.step(x_all, p_all, where=\"post\",\n",
    "                  color=\"black\", linestyle=\"--\", linewidth=2.0,\n",
    "                  label=\"__ALL__ (overall)\")[0]\n",
    "    handles.append(hov); labels.append(hov.get_label())\n",
    "    if SHOW_OVERALL_MEAN:\n",
    "        ax.axvline(np.mean(all_lengths), color=\"black\", linestyle=\"--\", linewidth=1.2, alpha=0.9)\n",
    "\n",
    "ax.set_xlabel(\"Sequence length (aa)\")\n",
    "ax.set_ylabel(\"Percent ≤ length\")\n",
    "ax.set_title(\"Cumulative Distribution of Sequence Lengths by Class\")\n",
    "if USE_LOG_X: ax.set_xscale(\"log\")\n",
    "ax.grid(True, alpha=0.3, linestyle=\":\")\n",
    "ax.legend(handles, labels, loc=\"best\", fontsize=9)  # ensure overall is included\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_PNG, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved plot to:\", SAVE_PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddb703",
   "metadata": {},
   "source": [
    "## Per Class ECDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328dcdfe-a272-4d8b-8130-1dfdcd6bcc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ECDF: one figure per class (with optional overall overlay) ===\n",
    "\n",
    "\n",
    "IN_DIR = RE_OG_UPD\n",
    "OUT_DIR_EACH = OUT_DIR / \"ecdf_per_class\"\n",
    "OUT_DIR_EACH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INCLUDE_OVERALL = True          # overlay pooled ECDF\n",
    "SHOW_MEAN = True                # vertical mean line (class)\n",
    "SHOW_MEDIAN = False             # vertical median line (class)\n",
    "SHOW_TRIMMED_MEAN = False       # vertical trimmed mean (25–75%)\n",
    "SHOW_OVERALL_MEAN = True        # vertical overall mean\n",
    "USE_LOG_X = False               # log-scale x axis if lengths span wide range\n",
    "\n",
    "def seq_len_fast(r): \n",
    "    return sum(1 for ch in str(r.seq) if ch not in \"-.\")\n",
    "\n",
    "def ecdf(vals):\n",
    "    if len(vals)==0: return np.array([]), np.array([])\n",
    "    x = np.sort(np.asarray(vals))\n",
    "    y = (np.arange(1, len(x)+1) / len(x)) * 100.0\n",
    "    return x, y\n",
    "\n",
    "def trimmed_mean(arr, lo=0.25, hi=0.75):\n",
    "    if len(arr) == 0: return np.nan\n",
    "    a = np.sort(np.asarray(arr))\n",
    "    i0, i1 = int(np.floor(len(a)*lo)), int(np.ceil(len(a)*hi))\n",
    "    mid = a[i0:i1]\n",
    "    return float(np.mean(mid)) if len(mid) else np.nan\n",
    "\n",
    "# Gather lengths\n",
    "lengths_by_class = {}\n",
    "all_lengths = []\n",
    "for f in sorted(Path(IN_DIR).glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    lens = [seq_len_fast(r) for r in SeqIO.parse(str(f), \"fasta\")]\n",
    "    if lens:\n",
    "        lengths_by_class[cls] = np.array(lens, dtype=int)\n",
    "        all_lengths.extend(lens)\n",
    "all_lengths = np.array(all_lengths, dtype=int)\n",
    "\n",
    "# Precompute overall ECDF\n",
    "x_all, p_all = ecdf(all_lengths) if len(all_lengths) else (np.array([]), np.array([]))\n",
    "overall_mean = float(np.mean(all_lengths)) if len(all_lengths) else np.nan\n",
    "\n",
    "for cls, lens in sorted(lengths_by_class.items()):\n",
    "    x, p = ecdf(lens)\n",
    "    if len(x) == 0:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    # Class ECDF\n",
    "    h = ax.step(x, p, where=\"post\", linewidth=2.0, label=f\"{cls} (n={len(lens)})\")[0]\n",
    "    color = h.get_color()\n",
    "\n",
    "    # Class markers\n",
    "    if SHOW_MEAN:\n",
    "        ax.axvline(np.mean(lens), color=color, linestyle=\":\", linewidth=1.2, alpha=0.9, label=\"mean\")\n",
    "    if SHOW_MEDIAN:\n",
    "        ax.axvline(np.median(lens), color=color, linestyle=\"-.\", linewidth=1.0, alpha=0.9, label=\"median\")\n",
    "    if SHOW_TRIMMED_MEAN:\n",
    "        ax.axvline(trimmed_mean(lens), color=color, linestyle=\"--\", linewidth=1.0, alpha=0.9, label=\"trimmed mean (25–75%)\")\n",
    "\n",
    "    # Overall overlay\n",
    "    if INCLUDE_OVERALL and len(x_all):\n",
    "        ax.step(x_all, p_all, where=\"post\", color=\"black\", linestyle=\"--\", linewidth=1.5, label=\"__ALL__ (overall)\")\n",
    "        if SHOW_OVERALL_MEAN and not np.isnan(overall_mean):\n",
    "            ax.axvline(overall_mean, color=\"black\", linestyle=\"--\", linewidth=1.0, alpha=0.9, label=\"overall mean\")\n",
    "\n",
    "    ax.set_xlabel(\"Sequence length (aa)\")\n",
    "    ax.set_ylabel(\"Percent ≤ length\")\n",
    "    ax.set_title(f\"ECDF of Sequence Lengths — {cls}\")\n",
    "    if USE_LOG_X:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.grid(True, alpha=0.3, linestyle=\":\")\n",
    "    ax.legend(loc=\"best\", fontsize=9)\n",
    "\n",
    "    out_png = OUT_DIR_EACH / f\"{cls}.length_ecdf.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c036d9-fd4f-434a-93eb-5e6c1efcb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct class imports (add these even if you imported earlier)\n",
    "\n",
    "# Find clustalo or muscle\n",
    "MSA_BIN = shutil.which(os.environ.get(\"CLUSTALO_BIN\") or \"clustalo\") or shutil.which(\"muscle\")\n",
    "assert MSA_BIN, \"Install clustalo or muscle (and/or set CLUSTALO_BIN).\"\n",
    "\n",
    "def write_class_fasta_for_alignment(in_path: Path, out_path: Path):\n",
    "    recs = []\n",
    "    for r in SeqIO.parse(str(in_path), \"fasta\"):\n",
    "        sid = first_token(r.id or r.description)\n",
    "        recs.append(SeqRecord(Seq(str(r.seq)), id=sid, description=\"\"))\n",
    "    write_fasta(recs, out_path)\n",
    "\n",
    "# Per-class alignments\n",
    "aln_paths = {}\n",
    "for f in sorted(RE_OG_UPD.glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    in_fa  = ALIGN_DIR / f\"{cls}.in.fasta\"\n",
    "    out_fa = ALIGN_DIR / f\"{cls}.aln.fasta\"\n",
    "    write_class_fasta_for_alignment(f, in_fa)\n",
    "\n",
    "    if \"clustalo\" in MSA_BIN:\n",
    "        cmd = [MSA_BIN, \"-i\", str(in_fa), \"-o\", str(out_fa), \"--force\", \"--threads=4\", \"--output-order=input-order\"]\n",
    "    else:  # muscle\n",
    "        cmd = [MSA_BIN, \"-align\", str(in_fa), \"-output\", str(out_fa)]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    aln_paths[cls] = out_fa\n",
    "\n",
    "# ALL combined\n",
    "all_in = ALIGN_DIR / \"__ALL__.in.fasta\"\n",
    "with open(all_in, \"w\") as out:\n",
    "    for f in sorted(RE_OG_UPD.glob(\"*.txt\")):\n",
    "        for r in SeqIO.parse(str(f), \"fasta\"):\n",
    "            sid = first_token(r.id or r.description)\n",
    "            SeqIO.write(SeqRecord(Seq(str(r.seq)), id=sid, description=\"\"), out, \"fasta\")\n",
    "\n",
    "all_out = ALIGN_DIR / \"__ALL__.aln.fasta\"\n",
    "if \"clustalo\" in MSA_BIN:\n",
    "    cmd = [MSA_BIN, \"-i\", str(all_in), \"-o\", str(all_out), \"--force\", \"--threads=4\", \"--output-order=input-order\"]\n",
    "else:\n",
    "    cmd = [MSA_BIN, \"-align\", str(all_in), \"-output\", str(all_out)]\n",
    "subprocess.run(cmd, check=True)\n",
    "aln_paths[\"__ALL__\"] = all_out\n",
    "\n",
    "aln_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb75c37",
   "metadata": {},
   "source": [
    "# Consensus-similarity heatmaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ba9ef-9fc9-4fba-93c4-c03636d2762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Consensus-similarity heatmaps (sequence vs position) ===\n",
    "\n",
    "# Reuse AA set if you want; not strictly needed here\n",
    "def majority_consensus_from_alignment(aln_fa: Path, threshold: float = 0.5) -> str:\n",
    "    \"\"\"Build majority-rule consensus directly from an alignment FASTA.\"\"\"\n",
    "    recs = list(SeqIO.parse(str(aln_fa), \"fasta\"))\n",
    "    assert recs, f\"Empty alignment: {aln_fa}\"\n",
    "    seqs = [str(r.seq) for r in recs]\n",
    "    L = max(len(s) for s in seqs)\n",
    "    cons = []\n",
    "    for j in range(L):\n",
    "        col = [s[j] if j < len(s) else \"-\" for s in seqs]\n",
    "        # ignore gaps\n",
    "        letters = [c for c in col if c not in \"-.\"]\n",
    "        if not letters:\n",
    "            cons.append(\"-\")\n",
    "            continue\n",
    "        # majority letter\n",
    "        vals, counts = np.unique(letters, return_counts=True)\n",
    "        i = int(np.argmax(counts))\n",
    "        frac = counts[i] / float(len(letters))\n",
    "        cons.append(vals[i] if frac >= threshold else \"X\")\n",
    "    return \"\".join(cons)\n",
    "\n",
    "def consensus_similarity_matrix(aln_fa: Path, consensus: str, ignore_when_cons_is_gap=True):\n",
    "    \"\"\"Return matrix (n_seq x L) with 1 if residue matches consensus, 0 if mismatch, nan if ignored.\"\"\"\n",
    "    recs = list(SeqIO.parse(str(aln_fa), \"fasta\"))\n",
    "    seqs = [str(r.seq) for r in recs]\n",
    "    L = len(consensus)\n",
    "    M = np.full((len(seqs), L), np.nan, dtype=float)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for j in range(L):\n",
    "            c_cons = consensus[j] if j < len(consensus) else \"-\"\n",
    "            c_seq  = s[j] if j < len(s) else \"-\"\n",
    "            if ignore_when_cons_is_gap and (c_cons in \"-.\"):\n",
    "                continue  # leave NaN\n",
    "            if c_seq in \"-.\":\n",
    "                continue  # leave NaN (gap in sequence)\n",
    "            M[i, j] = 1.0 if (c_seq == c_cons and c_cons not in {\"X\", \"-\"}) else 0.0\n",
    "    return recs, M\n",
    "def plot_similarity_heatmap(M: np.ndarray, title: str, save_path: Path = None, sort_by_overall=True):\n",
    "    \"\"\"Plot sequences × positions heatmap of matches to consensus.\"\"\"\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    H = M.copy()\n",
    "\n",
    "    # Optionally sort sequences (rows) by overall similarity to consensus\n",
    "    if sort_by_overall:\n",
    "        means = np.nanmean(H, axis=1)\n",
    "        order = np.argsort(-np.nan_to_num(means, nan=-1.0))\n",
    "        H = H[order, :]\n",
    "\n",
    "    # Mask NaNs so they render as blank\n",
    "    H_masked = np.ma.masked_invalid(H)\n",
    "\n",
    "    # Use modern colormap API (2 discrete levels: mismatch=0, match=1)\n",
    "    cmap = mpl.colormaps.get_cmap(\"viridis\").resampled(2)\n",
    "\n",
    "    plt.figure(figsize=(12, max(3, H.shape[0] * 0.03)))\n",
    "    im = plt.imshow(H_masked, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.xlabel(\"Alignment position\")\n",
    "    plt.ylabel(\"Sequences\")\n",
    "    plt.title(title)\n",
    "    cbar = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_ticklabels([\"mismatch\", \"match\"])\n",
    "\n",
    "    if save_path:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Generate heatmaps for each class and ALL\n",
    "for cls, aln in aln_paths.items():\n",
    "    cons = majority_consensus_from_alignment(aln, threshold=0.5)\n",
    "    recs, M = consensus_similarity_matrix(aln, cons, ignore_when_cons_is_gap=True)\n",
    "    out_png = HM_DIR / f\"{cls}.consensus_similarity.heatmap.png\"\n",
    "    plot_similarity_heatmap(M, f\"Consensus Similarity — {cls}\", save_path=out_png, sort_by_overall=True)\n",
    "    # Also save consensus used\n",
    "    with open(CONS_DIR / f\"{cls}.consensus.majority.fasta\", \"w\") as f:\n",
    "        f.write(f\">{cls}\\n\")\n",
    "        for i in range(0, len(cons), 80):\n",
    "            f.write(cons[i:i+80] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850730c9-8dfe-43bf-9a79-8ed300c4717e",
   "metadata": {},
   "source": [
    "# Expanded and Filtered Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41327249",
   "metadata": {},
   "source": [
    "## Imports and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c8f4a-a5f3-4589-9b04-02369e853187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Per-class length filtering into new FASTAs (range per class) ===\n",
    "# ----- SOURCE & OUTPUT -----\n",
    "SRC_DIR  = RE_OG_UPD                     # where your current per-class FASTAs (*.txt) live\n",
    "OUT_BASE = REBASE / \"filtered_sets_per_class\"   # base folder for outputs\n",
    "OUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----- CONFIG: specify ranges PER CLASS -----\n",
    "# Keys must match the class file stem (e.g., \"8ca.alpha.45\" for \"8ca.alpha.45.txt\").\n",
    "# Each class maps to:\n",
    "#   - one dict: {\"name\": \"...\", \"min_len\": L1, \"max_len\": L2}\n",
    "#   - OR a list of such dicts to create multiple outputs for that class.\n",
    "# Use None for open-ended bounds.\n",
    "CLASS_RANGE_SPECS = {\n",
    "    \"8ca.alpha.45\": {\"name\": \"alpha_no_change\", \"min_len\": None, \"max_len\": None},\n",
    "    \"8ca.beta.45\": {\"name\": \"beta_no_change\", \"min_len\": None, \"max_len\": None},\n",
    "    \"8ca.delta.45\": {\"name\": \"delta_250_600\", \"min_len\": 250, \"max_len\": 600},\n",
    "    \"8ca.eta.45\": {\"name\": \"eta_300_no_max\", \"min_len\": 300, \"max_len\": None},\n",
    "    \"8ca.gamma.45\": {\"name\": \"gamma_no_change\", \"min_len\": None, \"max_len\": None},\n",
    "    \"8ca.iota.45\": {\"name\": \"iota_no_change\", \"min_len\": None, \"max_len\": None},\n",
    "    \"8ca.theta.45\": {\"name\": \"theta_250_550\", \"min_len\": 250, \"max_len\": 550},\n",
    "    \"8ca.zeta.45\": {\"name\": \"zeta_150_250\", \"min_len\": 150, \"max_len\": 250}}\n",
    "\n",
    "# Only write files with at least this many kept sequences\n",
    "MIN_SEQS_TO_WRITE = 1\n",
    "\n",
    "# Whether to ignore '-' and '.' when computing length\n",
    "IGNORE_GAPS = True\n",
    "\n",
    "# ----- HELPERS -----\n",
    "def seq_len(rec: SeqRecord, ignore_gaps: bool = True) -> int:\n",
    "    s = str(rec.seq)\n",
    "    return sum(1 for ch in s if ch not in \"-.\") if ignore_gaps else len(s)\n",
    "\n",
    "def in_range(n: int, lo, hi) -> bool:\n",
    "    if lo is not None and n < lo: return False\n",
    "    if hi is not None and n > hi: return False\n",
    "    return True\n",
    "\n",
    "def ensure_list(spec):\n",
    "    if spec is None: return []\n",
    "    return spec if isinstance(spec, (list, tuple)) else [spec]\n",
    "\n",
    "# ----- MAIN -----\n",
    "class_files = sorted(Path(SRC_DIR).glob(\"*.txt\"))\n",
    "assert class_files, f\"No class FASTAs found in {SRC_DIR}\"\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# map stem -> path for quick lookup\n",
    "by_stem = {f.stem: f for f in class_files}\n",
    "\n",
    "# sanity: warn for spec keys not found\n",
    "missing = [k for k in CLASS_RANGE_SPECS.keys() if k not in by_stem]\n",
    "if missing:\n",
    "    print(\"[warn] Specified classes not found in SRC_DIR:\", \", \".join(missing))\n",
    "\n",
    "for cls, spec in CLASS_RANGE_SPECS.items():\n",
    "    if cls not in by_stem:\n",
    "        continue\n",
    "    src = by_stem[cls]\n",
    "    # load once\n",
    "    records = list(SeqIO.parse(str(src), \"fasta\"))\n",
    "    total = len(records)\n",
    "\n",
    "    for rng in ensure_list(spec):\n",
    "        name = rng.get(\"name\") or f\"{cls}_len_{rng.get('min_len')}_{rng.get('max_len')}\"\n",
    "        lo   = rng.get(\"min_len\", None)\n",
    "        hi   = rng.get(\"max_len\", None)\n",
    "\n",
    "        out_dir = OUT_BASE / name\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        kept = []\n",
    "        for r in records:\n",
    "            L = seq_len(r, IGNORE_GAPS)\n",
    "            if in_range(L, lo, hi):\n",
    "                rid = (r.id or r.description or \"\").split()[0]\n",
    "                kept.append(SeqRecord(Seq(str(r.seq)), id=rid, description=\"\"))\n",
    "\n",
    "        if len(kept) >= MIN_SEQS_TO_WRITE:\n",
    "            out_fa = out_dir / f\"{cls}.txt\"\n",
    "            SeqIO.write(kept, str(out_fa), \"fasta\")\n",
    "            out_path = str(out_fa)\n",
    "        else:\n",
    "            out_path = \"\"\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"class\": cls,\n",
    "            \"range_name\": name,\n",
    "            \"min_len\": lo,\n",
    "            \"max_len\": hi,\n",
    "            \"n_total_in_src\": total,\n",
    "            \"n_kept\": len(kept),\n",
    "            \"out_path\": out_path,\n",
    "        })\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"class\", \"range_name\"])\n",
    "summary_csv = OUT_BASE / \"per_class_filtered_summary.csv\"\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "\n",
    "print(f\"[OK] Wrote filtered FASTAs under: {OUT_BASE}\")\n",
    "print(f\"[OK] Summary → {summary_csv}\")\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c63b5-164e-43af-896b-d406faaec62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Length stats from multiple per-class folders (you specify the dirs) ===\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG:folders here ----------------\n",
    "DIRS = [\n",
    "    REBASE / \"filtered_sets_per_class\" / \"alpha_no_change\",\n",
    "    REBASE / \"filtered_sets_per_class\" / \"beta_no_change\",     \n",
    "    REBASE / \"filtered_sets_per_class\" / \"gamma_no_change\",     \n",
    "    REBASE / \"filtered_sets_per_class\" / \"delta_250_600\",\n",
    "    REBASE / \"filtered_sets_per_class\" / \"zeta_150_250\",       \n",
    "    REBASE / \"filtered_sets_per_class\" / \"eta_300_no_max\",       \n",
    "    REBASE / \"filtered_sets_per_class\" / \"theta_250_550\",      \n",
    "    REBASE / \"filtered_sets_per_class\" / \"iota_no_change\",      \n",
    "]\n",
    "\n",
    "OUT_CSV = OUT_DIR / \"length_stats_selected_dirs.csv\"\n",
    "\n",
    "# If the same class appears in multiple dirs:\n",
    "#   - True  => merge sequences across files, dedup by ID\n",
    "#   - False => raise an error if a class is seen more than once\n",
    "MERGE_DUPLICATE_CLASSES = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def first_token(s: str) -> str:\n",
    "    return (s or \"\").split()[0]\n",
    "\n",
    "def seq_len(rec: SeqRecord) -> int:\n",
    "    # length ignoring gaps\n",
    "    return sum(1 for ch in str(rec.seq) if ch not in \"-.\")\n",
    "\n",
    "def trimmed_mean(arr, lo=0.25, hi=0.75):\n",
    "    if not arr: return float(\"nan\")\n",
    "    a = np.array(sorted(arr))\n",
    "    i0, i1 = int(np.floor(len(a)*lo)), int(np.ceil(len(a)*hi))\n",
    "    mid = a[i0:i1]\n",
    "    return float(np.mean(mid)) if len(mid) else float(\"nan\")\n",
    "\n",
    "def trimmed_median(arr, lo=0.25, hi=0.75):\n",
    "    if not arr: return float(\"nan\")\n",
    "    a = np.array(sorted(arr))\n",
    "    i0, i1 = int(np.floor(len(a)*lo)), int(np.ceil(len(a)*hi))\n",
    "    mid = a[i0:i1]\n",
    "    return float(np.median(mid)) if len(mid) else float(\"nan\")\n",
    "\n",
    "# ---------------- Collect class files from the specified dirs ----------------\n",
    "files = []\n",
    "for d in DIRS:\n",
    "    d = Path(d)\n",
    "    assert d.exists(), f\"Directory does not exist: {d}\"\n",
    "    for ext in (\"*.txt\", \"*.fa\", \"*.fasta\", \"*.faa\", \"*.fas\"):\n",
    "        files.extend(sorted(d.glob(ext)))\n",
    "\n",
    "assert files, f\"No FASTA files found in: {', '.join(str(Path(d)) for d in DIRS)}\"\n",
    "\n",
    "# ---------------- Load per-class sequences (merge or enforce uniqueness) ----------------\n",
    "class_to_ids = {}\n",
    "class_to_lengths = {}\n",
    "\n",
    "for fp in files:\n",
    "    cls = fp.stem  # e.g., '8ca.alpha.45'\n",
    "    # Load and normalize IDs\n",
    "    rec_ids = []\n",
    "    for r in SeqIO.parse(str(fp), \"fasta\"):\n",
    "        rid = first_token(r.id or r.description)\n",
    "        rec_ids.append((rid, seq_len(r)))\n",
    "\n",
    "    if cls not in class_to_ids:\n",
    "        class_to_ids[cls] = set()\n",
    "        class_to_lengths[cls] = []\n",
    "\n",
    "    if not MERGE_DUPLICATE_CLASSES and class_to_lengths[cls]:\n",
    "        raise RuntimeError(f\"Class {cls} appears in multiple input folders; set MERGE_DUPLICATE_CLASSES=True to merge.\")\n",
    "\n",
    "    # Merge (dedup by ID)\n",
    "    seen = class_to_ids[cls]\n",
    "    for rid, L in rec_ids:\n",
    "        if rid in seen:\n",
    "            continue\n",
    "        seen.add(rid)\n",
    "        class_to_lengths[cls].append(L)\n",
    "\n",
    "# ---------------- Compute stats ----------------\n",
    "rows = []\n",
    "all_lengths = []\n",
    "\n",
    "for cls, lens in sorted(class_to_lengths.items()):\n",
    "    if len(lens) == 0:\n",
    "        rows.append({\n",
    "            \"class\": cls, \"n_seqs\": 0,\n",
    "            \"mean_len\": float(\"nan\"), \"median_len\": float(\"nan\"),\n",
    "            \"std_len\": float(\"nan\"),\n",
    "            \"trimmed_mean_len_25_75\": float(\"nan\"),\n",
    "            \"trimmed_median_len_25_75\": float(\"nan\"),\n",
    "            \"min_len\": None, \"q25_len\": None, \"q75_len\": None, \"max_len\": None,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    all_lengths.extend(lens)\n",
    "    lens_arr = np.array(lens, dtype=float)\n",
    "    rows.append({\n",
    "        \"class\": cls,\n",
    "        \"n_seqs\": int(len(lens_arr)),\n",
    "        \"mean_len\": float(np.mean(lens_arr)),\n",
    "        \"median_len\": float(np.median(lens_arr)),\n",
    "        \"std_len\": float(np.std(lens_arr, ddof=1)) if len(lens_arr) > 1 else float(\"nan\"),\n",
    "        \"trimmed_mean_len_25_75\": trimmed_mean(lens),\n",
    "        \"trimmed_median_len_25_75\": trimmed_median(lens),\n",
    "        \"min_len\": int(np.min(lens_arr)),\n",
    "        \"q25_len\": int(np.quantile(lens_arr, 0.25)),\n",
    "        \"q75_len\": int(np.quantile(lens_arr, 0.75)),\n",
    "        \"max_len\": int(np.max(lens_arr)),\n",
    "    })\n",
    "\n",
    "# Overall row\n",
    "if all_lengths:\n",
    "    all_arr = np.array(all_lengths, dtype=float)\n",
    "    rows.append({\n",
    "        \"class\": \"__ALL__\",\n",
    "        \"n_seqs\": int(sum(r[\"n_seqs\"] for r in rows)),\n",
    "        \"mean_len\": float(np.mean(all_arr)),\n",
    "        \"median_len\": float(np.median(all_arr)),\n",
    "        \"std_len\": float(np.std(all_arr, ddof=1)) if len(all_arr) > 1 else float(\"nan\"),\n",
    "        \"trimmed_mean_len_25_75\": trimmed_mean(all_lengths),\n",
    "        \"trimmed_median_len_25_75\": trimmed_median(all_lengths),\n",
    "        \"min_len\": int(np.min(all_arr)),\n",
    "        \"q25_len\": int(np.quantile(all_arr, 0.25)),\n",
    "        \"q75_len\": int(np.quantile(all_arr, 0.75)),\n",
    "        \"max_len\": int(np.max(all_arr)),\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(rows).sort_values(\"class\")\n",
    "df_stats.to_csv(OUT_CSV, index=False)\n",
    "print(\"Dirs used:\")\n",
    "for d in DIRS: print(\" -\", d)\n",
    "print(\"Saved:\", OUT_CSV)\n",
    "df_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c7598",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546186c4-b985-412a-a3a1-25bc0a07f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ECDF (length distributions) from multiple filtered-set directories ===\n",
    "\n",
    "SAVE_PNG = OUT_DIR / \"length_ecdf_combined_filtered_dirs.png\"\n",
    "INCLUDE_OVERALL = True\n",
    "MAX_CLASSES_PLOTTED = None\n",
    "USE_LOG_X = False\n",
    "SHOW_MEANS = True\n",
    "SHOW_OVERALL_MEAN = True\n",
    "MERGE_DUPLICATES = True  # dedup if same class appears in multiple dirs\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def first_token(s: str) -> str:\n",
    "    return (s or \"\").split()[0]\n",
    "\n",
    "def seq_len_fast(r): \n",
    "    return sum(1 for ch in str(r.seq) if ch not in \"-.\")\n",
    "\n",
    "def ecdf(vals):\n",
    "    if len(vals)==0: return np.array([]), np.array([])\n",
    "    x = np.sort(np.asarray(vals))\n",
    "    y = (np.arange(1, len(x)+1) / len(x)) * 100.0\n",
    "    return x, y\n",
    "\n",
    "# ---------------- Collect lengths per class ----------------\n",
    "lengths_by_class = {}\n",
    "ids_by_class = {}\n",
    "all_lengths = []\n",
    "\n",
    "for d in DIRS:\n",
    "    d = Path(d)\n",
    "    assert d.exists(), f\"Missing directory: {d}\"\n",
    "    for ext in (\"*.txt\", \"*.fa\", \"*.fasta\", \"*.faa\", \"*.fas\"):\n",
    "        for f in sorted(d.glob(ext)):\n",
    "            cls = f.stem\n",
    "            for r in SeqIO.parse(str(f), \"fasta\"):\n",
    "                rid = first_token(r.id or r.description)\n",
    "                L = seq_len_fast(r)\n",
    "                if MERGE_DUPLICATES:\n",
    "                    if cls not in ids_by_class:\n",
    "                        ids_by_class[cls] = set()\n",
    "                        lengths_by_class[cls] = []\n",
    "                    if rid in ids_by_class[cls]:\n",
    "                        continue\n",
    "                    ids_by_class[cls].add(rid)\n",
    "                    lengths_by_class[cls].append(L)\n",
    "                else:\n",
    "                    lengths_by_class.setdefault(cls, []).append(L)\n",
    "\n",
    "# Merge all lengths into one array\n",
    "for lens in lengths_by_class.values():\n",
    "    all_lengths.extend(lens)\n",
    "\n",
    "# Optionally limit plotted classes\n",
    "if MAX_CLASSES_PLOTTED is not None and len(lengths_by_class) > MAX_CLASSES_PLOTTED:\n",
    "    keep = dict(sorted(lengths_by_class.items(), key=lambda kv: len(kv[1]), reverse=True)[:MAX_CLASSES_PLOTTED])\n",
    "    lengths_by_class = keep\n",
    "\n",
    "# ---------------- Plot ----------------\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "handles, labels = [], []\n",
    "\n",
    "for cls, lens in sorted(lengths_by_class.items()):\n",
    "    if not lens:\n",
    "        continue\n",
    "    x, p = ecdf(lens)\n",
    "    h = ax.step(x, p, where=\"post\", linewidth=1.3, label=f\"{cls} (n={len(lens)})\")[0]\n",
    "    handles.append(h); labels.append(h.get_label())\n",
    "    if SHOW_MEANS:\n",
    "        ax.axvline(np.mean(lens), color=h.get_color(), linestyle=\":\", linewidth=1.0, alpha=0.8)\n",
    "\n",
    "# Overall ECDF\n",
    "if INCLUDE_OVERALL and len(all_lengths) > 0:\n",
    "    x_all, p_all = ecdf(all_lengths)\n",
    "    hov = ax.step(x_all, p_all, where=\"post\",\n",
    "                  color=\"black\", linestyle=\"--\", linewidth=2.0,\n",
    "                  label=\"__ALL__ (overall)\")[0]\n",
    "    handles.append(hov); labels.append(hov.get_label())\n",
    "    if SHOW_OVERALL_MEAN:\n",
    "        ax.axvline(np.mean(all_lengths), color=\"black\", linestyle=\"--\", linewidth=1.2, alpha=0.9)\n",
    "\n",
    "ax.set_xlabel(\"Sequence length (aa)\")\n",
    "ax.set_ylabel(\"Percent ≤ length\")\n",
    "ax.set_title(\"Cumulative Distribution of Sequence Lengths — Combined Filtered Sets\")\n",
    "if USE_LOG_X:\n",
    "    ax.set_xscale(\"log\")\n",
    "ax.grid(True, alpha=0.3, linestyle=\":\")\n",
    "ax.legend(handles, labels, loc=\"best\", fontsize=9)\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_PNG, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved plot → {SAVE_PNG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d49d88-477d-4362-90dd-a73a75f09be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build alignments from multiple filtered-set folders, then make consensus-similarity heatmaps ===\n",
    "\n",
    "# ---------- OUTPUT locations (re-use if you already set these elsewhere) ----------\n",
    "OUT_DIR   = OUT_DIR if 'OUT_DIR' in globals() else (REBASE / \"idr_length_consensus_outputs\")\n",
    "ALIGN_DIR = ALIGN_DIR if 'ALIGN_DIR' in globals() else (OUT_DIR / \"aligned\")\n",
    "CONS_DIR  = CONS_DIR  if 'CONS_DIR'  in globals() else (OUT_DIR / \"consensus\")\n",
    "HM_DIR    = HM_DIR    if 'HM_DIR'    in globals() else (OUT_DIR / \"heatmaps\")\n",
    "for d in [OUT_DIR, ALIGN_DIR, CONS_DIR, HM_DIR]:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- MSA tool ----------\n",
    "MSA_BIN = shutil.which(os.environ.get(\"CLUSTALO_BIN\") or \"clustalo\") or shutil.which(\"muscle\")\n",
    "assert MSA_BIN, \"Install clustalo or muscle (and/or set CLUSTALO_BIN).\"\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def first_token(s: str) -> str:\n",
    "    return (s or \"\").split()[0]\n",
    "\n",
    "def write_fasta(records, path: Path):\n",
    "    if records:\n",
    "        SeqIO.write(records, str(path), \"fasta\")\n",
    "\n",
    "def run_msa(in_fa: Path, out_fa: Path, threads: int = 4):\n",
    "    if \"clustalo\" in MSA_BIN:\n",
    "        cmd = [MSA_BIN, \"-i\", str(in_fa), \"-o\", str(out_fa), \"--force\", f\"--threads={threads}\", \"--output-order=input-order\"]\n",
    "    else:  # muscle\n",
    "        cmd = [MSA_BIN, \"-align\", str(in_fa), \"-output\", str(out_fa)]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "# ---------- 1) Gather sequences per class from all DIRS (merge + dedup by ID) ----------\n",
    "class_to_records = {}   # stem -> dict(id -> SeqRecord)\n",
    "for d in DIRS:\n",
    "    d = Path(d)\n",
    "    assert d.exists(), f\"Missing directory: {d}\"\n",
    "    for ext in (\"*.txt\",\"*.fa\",\"*.fasta\",\"*.faa\",\"*.fas\"):\n",
    "        for f in d.glob(ext):\n",
    "            cls = f.stem  # e.g., \"8ca.alpha.45\"\n",
    "            bucket = class_to_records.setdefault(cls, {})\n",
    "            for r in SeqIO.parse(str(f), \"fasta\"):\n",
    "                rid = first_token(r.id or r.description)\n",
    "                if rid in bucket:\n",
    "                    continue\n",
    "                bucket[rid] = SeqRecord(Seq(str(r.seq)), id=rid, description=\"\")\n",
    "\n",
    "# ---------- 2) Build per-class MSAs ----------\n",
    "aln_paths = {}\n",
    "for cls, id2rec in sorted(class_to_records.items()):\n",
    "    recs = list(id2rec.values())\n",
    "    if not recs:\n",
    "        continue\n",
    "    in_fa  = Path(ALIGN_DIR) / f\"{cls}.in.fasta\"\n",
    "    out_fa = Path(ALIGN_DIR) / f\"{cls}.aln.fasta\"\n",
    "    write_fasta(recs, in_fa)\n",
    "    run_msa(in_fa, out_fa)\n",
    "    aln_paths[cls] = out_fa\n",
    "\n",
    "# ---------- 3) Build ALL combined MSA ----------\n",
    "all_in  = Path(ALIGN_DIR) / \"__ALL__.in.fasta\"\n",
    "all_out = Path(ALIGN_DIR) / \"__ALL__.aln.fasta\"\n",
    "all_recs = [rec for id2rec in class_to_records.values() for rec in id2rec.values()]\n",
    "write_fasta(all_recs, all_in)\n",
    "run_msa(all_in, all_out)\n",
    "aln_paths[\"__ALL__\"] = all_out\n",
    "\n",
    "# ---------- 4) Consensus + similarity matrix + heatmap (same as your existing logic, bundled here) ----------\n",
    "def majority_consensus_from_alignment(aln_fa: Path, threshold: float = 0.5) -> str:\n",
    "    \"\"\"Build majority-rule consensus directly from an alignment FASTA.\"\"\"\n",
    "    recs = list(SeqIO.parse(str(aln_fa), \"fasta\"))\n",
    "    assert recs, f\"Empty alignment: {aln_fa}\"\n",
    "    seqs = [str(r.seq) for r in recs]\n",
    "    L = max(len(s) for s in seqs)\n",
    "    cons = []\n",
    "    for j in range(L):\n",
    "        col = [s[j] if j < len(s) else \"-\" for s in seqs]\n",
    "        letters = [c for c in col if c not in \"-.\"]\n",
    "        if not letters:\n",
    "            cons.append(\"-\"); continue\n",
    "        vals, counts = np.unique(letters, return_counts=True)\n",
    "        i = int(np.argmax(counts))\n",
    "        frac = counts[i] / float(len(letters))\n",
    "        cons.append(vals[i] if frac >= threshold else \"X\")\n",
    "    return \"\".join(cons)\n",
    "\n",
    "def consensus_similarity_matrix(aln_fa: Path, consensus: str, ignore_when_cons_is_gap=True):\n",
    "    \"\"\"Return M (n_seq x L): 1 if residue matches consensus, 0 if mismatch, NaN if ignored (gap/cons-gap).\"\"\"\n",
    "    recs = list(SeqIO.parse(str(aln_fa), \"fasta\"))\n",
    "    seqs = [str(r.seq) for r in recs]\n",
    "    L = len(consensus)\n",
    "    M = np.full((len(seqs), L), np.nan, dtype=float)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for j in range(L):\n",
    "            c_cons = consensus[j] if j < len(consensus) else \"-\"\n",
    "            c_seq  = s[j] if j < len(s) else \"-\"\n",
    "            if ignore_when_cons_is_gap and (c_cons in \"-.\"):\n",
    "                continue\n",
    "            if c_seq in \"-.\":\n",
    "                continue\n",
    "            M[i, j] = 1.0 if (c_seq == c_cons and c_cons not in {\"X\",\"-\"}) else 0.0\n",
    "    return recs, M\n",
    "\n",
    "def plot_similarity_heatmap(M: np.ndarray, title: str, save_path: Path = None, sort_by_overall=True):\n",
    "    import matplotlib as mpl\n",
    "    H = M.copy()\n",
    "    if sort_by_overall:\n",
    "        means = np.nanmean(H, axis=1)\n",
    "        order = np.argsort(-np.nan_to_num(means, nan=-1.0))\n",
    "        H = H[order, :]\n",
    "    H_masked = np.ma.masked_invalid(H)\n",
    "    cmap = mpl.colormaps.get_cmap(\"viridis\").resampled(2)\n",
    "    plt.figure(figsize=(12, max(3, H.shape[0] * 0.03)))\n",
    "    im = plt.imshow(H_masked, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.xlabel(\"Alignment position\"); plt.ylabel(\"Sequences\"); plt.title(title)\n",
    "    cbar = plt.colorbar(im, fraction=0.046, pad=0.04); cbar.set_ticks([0, 1]); cbar.set_ticklabels([\"mismatch\", \"match\"])\n",
    "    if save_path:\n",
    "        plt.tight_layout(); plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------- 5) Run heatmaps for each class + ALL ----------\n",
    "for cls, aln in aln_paths.items():\n",
    "    cons = majority_consensus_from_alignment(aln, threshold=0.5)\n",
    "    _, M = consensus_similarity_matrix(aln, cons, ignore_when_cons_is_gap=True)\n",
    "    out_png = Path(HM_DIR) / f\"{cls}.consensus_similarity.heatmap.png\"\n",
    "    plot_similarity_heatmap(M, f\"Consensus Similarity — {cls}\", save_path=out_png, sort_by_overall=True)\n",
    "    # save consensus\n",
    "    with open(Path(CONS_DIR) / f\"{cls}.consensus.majority.fasta\", \"w\") as f:\n",
    "        f.write(f\">{cls}\\n\")\n",
    "        for i in range(0, len(cons), 80):\n",
    "            f.write(cons[i:i+80] + \"\\n\")\n",
    "\n",
    "print(\"[OK] Built alignments from DIRS and saved consensus-similarity heatmaps to:\", HM_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hmmer_env)",
   "language": "python",
   "name": "hmmer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
